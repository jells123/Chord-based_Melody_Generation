{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 13111996\n",
      "python 3.6.8 (default, Aug 20 2019, 17:12:48) \n",
      "[GCC 8.3.0]\n",
      "keras version 2.4.0\n",
      "tensorflow version 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import data_utils\n",
    "import pred_utils\n",
    "import models\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'storage'\n",
    "ARTIST = \"zun\"\n",
    "REP = 1\n",
    "\n",
    "NOTES_WINDOW = 8\n",
    "CHORDS_WINDOW = 3\n",
    "PITCHES_PER_CHORD = 4\n",
    "MODE = \"embeddings\"\n",
    "\n",
    "MIN_FREQ = 5 * REP\n",
    "UNKNOWN = \"UNK\"\n",
    "\n",
    "VALIDATION_SIZE = 0.1\n",
    "\n",
    "data = data_utils.get_parsed_data(PATH)\n",
    "chords_mapping = data_utils.get_chords_mapping(PATH)\n",
    "\n",
    "NONECLASS = False\n",
    "indices_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Load notes embeddings: embeddings_NOTES_size75_window8_CBOW_H-SFMAX.bin\n",
      "> Load chords embeddings: embeddings_CHORDS-SHORT_size100_window2_CBOW_NEG-S.bin\n"
     ]
    }
   ],
   "source": [
    "if MODE == 'embeddings' or MODE == \"custom\":\n",
    "    EMBEDDINGS = data_utils.load_embeddings(PATH, embeddings_dir=\"embeddings\")\n",
    "    NOTES_EMB_SIZE = EMBEDDINGS[\"NOTES\"].vector_size\n",
    "    CHORDS_EMB_SIZE = EMBEDDINGS[\"CHORDS\"].vector_size\n",
    "else:\n",
    "    EMBEDDINGS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TEST indices\n",
      "222 1317\n",
      "No. UNK chords: 0\n",
      "pitches: (12070, 43)\n",
      "durations: (12070, 20)\n",
      "# TRAIN SONGS: 198\n",
      "# VALIDATION SONGS: 22\n",
      "NO SCALING :)\n"
     ]
    }
   ],
   "source": [
    "TEST_SELECT = data_utils.select_test(data, PATH)\n",
    "TRAIN_SELECT = data_utils.select_train(data, artist=ARTIST, test_select=TEST_SELECT, chorus_verse_only=not isinstance(ARTIST, list))\n",
    "\n",
    "TRAIN_SONGS, notes_translated, chords_translated = data_utils.select_songs(data, TRAIN_SELECT, REPEAT=1)\n",
    "TEST_SONGS, notes_translated_test, chords_translated_test = data_utils.select_songs(data, TEST_SELECT, REPEAT=1)\n",
    "print(len(TRAIN_SONGS), len(TEST_SONGS))\n",
    "\n",
    "X_train_n_raw, X_train_ch_raw, y_train_raw, is_start_train, row_to_song_idx_train = data_utils.prepare_raw_X_y(\n",
    "    notes_translated, \n",
    "    chords_translated,\n",
    "    notes_window=NOTES_WINDOW,\n",
    "    chords_window=CHORDS_WINDOW,\n",
    "    use_next_chord=True,\n",
    "    rep=REP\n",
    ")\n",
    "\n",
    "y_train_raw_pitches, y_train_raw_durations = data_utils._split_notes(y_train_raw)\n",
    "y_train_raw_pitches, y_train_raw_durations = data_utils.trim_rare_labels_separately(\n",
    "    y_train_raw_pitches, y_train_raw_durations, \n",
    "    min_freq=MIN_FREQ, \n",
    "    unk=UNKNOWN\n",
    ")\n",
    "\n",
    "X_train, indices_cache = data_utils.prepare_input_X(\n",
    "    X_train_n_raw, \n",
    "    X_train_ch_raw, \n",
    "    is_start_train, \n",
    "    row_to_song_idx_train, \n",
    "    mode=MODE,\n",
    "    notes_window=NOTES_WINDOW, \n",
    "    chords_window=CHORDS_WINDOW,\n",
    "    chord_size=PITCHES_PER_CHORD,\n",
    "    chords_mapping=chords_mapping,\n",
    "    embeddings=EMBEDDINGS,\n",
    "    indices_cache=indices_cache\n",
    ")\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "y_train_pitches, token_to_id_pitches = data_utils.prepare_input_y(y_train_raw_pitches, noneclass=NONECLASS)\n",
    "y_train_durations, token_to_id_durations = data_utils.prepare_input_y(y_train_raw_durations, noneclass=NONECLASS)\n",
    "\n",
    "id_to_token_pitches = {token_to_id_pitches[token] : token for token in token_to_id_pitches}\n",
    "id_to_token_durations = {token_to_id_durations[token] : token for token in token_to_id_durations}\n",
    "\n",
    "print(\"pitches:\", y_train_pitches.shape)\n",
    "print(\"durations:\", y_train_durations.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "song_idx_list = list(set(row_to_song_idx_train))\n",
    "train_idx_list, validation_idx_list = train_test_split(song_idx_list, train_size=1-VALIDATION_SIZE, test_size=VALIDATION_SIZE, random_state=20202208)   \n",
    "\n",
    "print(\"# TRAIN SONGS:\", len(train_idx_list))\n",
    "print(\"# VALIDATION SONGS:\", len(validation_idx_list))\n",
    "\n",
    "# _________________ NEW FEATURE = SCALING\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X_train.reshape(X_train.shape[0], -1))\n",
    "\n",
    "columns_to_scale = [i for i in range(NOTES_EMB_SIZE*NOTES_WINDOW, NOTES_EMB_SIZE*NOTES_WINDOW+NOTES_WINDOW)]\n",
    "off = NOTES_EMB_SIZE*NOTES_WINDOW+NOTES_WINDOW\n",
    "off += CHORDS_EMB_SIZE*(CHORDS_WINDOW+1)\n",
    "columns_to_scale.extend([i for i in range(off, off+CHORDS_WINDOW+1+1)])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "df[columns_to_scale] = mms.fit_transform(df[columns_to_scale])\n",
    "\n",
    "X_train_reshaped = df.values.reshape(X_train.shape[0], 1, -1)\n",
    "\n",
    "#  _________________ END OF NEW FEATURE\n",
    "\n",
    "if not NONECLASS:\n",
    "    print(\"NO SCALING :)\")\n",
    "    X_train_reshaped = X_train.reshape(X_train.shape[0], 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = data_utils.get_bigram_transitions_chord_wise(notes_translated, chords_translated, translate_pitch_to_name=True)\n",
    "with open(os.path.join(PATH, \"models\", \"transitions\", f\"{ARTIST}_transitions.json\"), \"w\") as handle:\n",
    "    json.dump(transitions, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage/models/embeddings/LSTM-512_*\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "batch_input_shape = (BATCH_SIZE, 1, X_train.shape[1])\n",
    "num_pitches = y_train_pitches.shape[-1]\n",
    "num_durations = y_train_durations.shape[-1]\n",
    "\n",
    "LSTM_SIZE = 512 if ARTIST == \"*\" else 128\n",
    "LEARNING_RATE = 1e-5\n",
    "model, opt, model_name = models.get_stateful_lstm_multi_output(batch_input_shape, num_pitches, num_durations, LSTM_SIZE, lr=LEARNING_RATE)\n",
    "# model, opt, model_name = models.get_stateful_lstm_multi_output_deeper_NEW(batch_input_shape, num_pitches, num_durations, LSTM_SIZE, LSTM_SIZE, lr=LEARNING_RATE)\n",
    "\n",
    "model_name += f\"_{ARTIST}\"\n",
    "model_dir = os.path.join(PATH, \"models\", MODE, model_name)\n",
    "print(model_dir)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "predictions_dir = os.path.join(model_dir, \"predictions\")\n",
    "if not os.path.exists(predictions_dir):\n",
    "    os.makedirs(predictions_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to open file (unable to open file: name = 'storage/models/embeddings/LSTM-512_*/weights_51.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(model_dir, \"loss.json\"), \"r\") as handle:\n",
    "    loss_per_epoch = json.load(handle)\n",
    "    epochs_num = len(loss_per_epoch[\"train\"][\"all\"])\n",
    "#     epochs_num = 50\n",
    "    \n",
    "try:\n",
    "    model.load_weights(os.path.join(model_dir, f'weights_{epochs_num}.hdf5'))\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    model.load_weights(os.path.join(model_dir, f'weights-latest.hdf5'))\n",
    "epochs_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A minor triad_2.0\n",
      "D5_0.5\n",
      "A quartal trichord_2.0\n",
      "G5_0.75\n",
      "A quartal trichord_2.0\n",
      "G5_0.75\n",
      "A quartal trichord_2.0\n",
      "G5_0.75\n",
      "A minor triad_2.0\n",
      "G#4_0.5\n",
      "A minor triad_2.0\n",
      "D4_0.5\n",
      "A minor triad_2.0\n",
      "D5_0.5\n",
      "C major triad_2.0\n",
      "REST_0.25\n",
      "C major triad_2.0\n",
      "B6_0.5\n",
      "C major triad_2.0\n",
      "E5_0.75\n",
      "C major triad_2.0\n",
      "E6_0.5\n",
      "D major triad_4.0\n",
      "REST_0.25\n",
      "D major triad_4.0\n",
      "C4_0.75\n",
      "D major triad_4.0\n",
      "F5_0.5\n",
      "D major triad_4.0\n",
      "G6_0.5\n",
      "F major triad_2.0\n",
      "B-4_2.0\n",
      "F major triad_2.0\n",
      "REST_0.25\n",
      "F major triad_2.0\n",
      "A3_1.5\n",
      "F major triad_2.0\n",
      "G4_0.75\n",
      "G major triad_2.0\n",
      "C6_1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'storage/models/embeddings/LSTM-NEW-128-128_zun/predictions/prob_sample_T=0.2_2020-11-28_13:14:13.mid'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chords = chords_translated[0]\n",
    "\n",
    "ch_idx = 0\n",
    "seed_notes = []\n",
    "note_t, chord_t = 0.0, 0.0\n",
    "prev_note = None\n",
    "\n",
    "TEMPERATURE = 1.0\n",
    "# while len(seed_notes) < NOTES_WINDOW:\n",
    "while True:\n",
    "    current_chord = chords[ch_idx]\n",
    "    print(current_chord)\n",
    "    if not prev_note:\n",
    "        tr_select = list(transitions[current_chord][\"START_0.0\"].items())\n",
    "    else:\n",
    "        tr_select = list(transitions[current_chord][prev_note].items())\n",
    "        \n",
    "    notes = [pair[0] for pair in tr_select]\n",
    "    probs = np.array([pair[1] for pair in tr_select])\n",
    "    probs /= TEMPERATURE\n",
    "    probs = probs / sum(probs)\n",
    "    \n",
    "    next_note = np.random.choice(notes, p=probs)\n",
    "    seed_notes.append(next_note)\n",
    "    print(next_note)\n",
    "    \n",
    "    note_t += float(next_note.split('_')[-1])\n",
    "    while chord_t < note_t:\n",
    "        chord_t += float(chords[ch_idx].split('_')[-1])\n",
    "        ch_idx += 1\n",
    "        if ch_idx == len(chords):\n",
    "            break\n",
    "        else: \n",
    "            current_chord = chords[ch_idx]\n",
    "            \n",
    "    if ch_idx == len(chords):\n",
    "        break\n",
    "  \n",
    "midi_stream = pred_utils.newsong_to_midistream(seed_notes, chords, tr_chords_dict=chords_mapping)\n",
    "out_fnam = f\"prob_sample_T={'None' if not TEMPERATURE else TEMPERATURE}_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}.mid\"\n",
    "midi_stream.write(\n",
    "    'midi', \n",
    "    fp=os.path.join(predictions_dir, out_fnam)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "datasets/pianoroll/j/jukebox-the-ghost/the-stars/chorus_nokey.mid\n",
      "datasets/pianoroll/t/they-might-be-giants/weep-day/verse_nokey.mid\n",
      "datasets/pianoroll/r/radiohead/pyramid-song/verse_nokey.mid\n",
      "datasets/pianoroll/j/jin/yobanashi-deceive/pre-chorus_nokey.mid\n",
      "datasets/pianoroll/l/lim-kim/awoo/pre-chorus_nokey.mid\n",
      "0.1\n",
      "datasets/pianoroll/j/jukebox-the-ghost/the-stars/chorus_nokey.mid\n",
      "datasets/pianoroll/t/they-might-be-giants/weep-day/verse_nokey.mid\n",
      "datasets/pianoroll/r/radiohead/pyramid-song/verse_nokey.mid\n",
      "datasets/pianoroll/j/jin/yobanashi-deceive/pre-chorus_nokey.mid\n",
      "datasets/pianoroll/l/lim-kim/awoo/pre-chorus_nokey.mid\n",
      "0.25\n",
      "datasets/pianoroll/j/jukebox-the-ghost/the-stars/chorus_nokey.mid\n",
      "datasets/pianoroll/t/they-might-be-giants/weep-day/verse_nokey.mid\n",
      "datasets/pianoroll/r/radiohead/pyramid-song/verse_nokey.mid\n",
      "datasets/pianoroll/j/jin/yobanashi-deceive/pre-chorus_nokey.mid\n",
      "datasets/pianoroll/l/lim-kim/awoo/pre-chorus_nokey.mid\n",
      "0.75\n",
      "datasets/pianoroll/j/jukebox-the-ghost/the-stars/chorus_nokey.mid\n",
      "datasets/pianoroll/t/they-might-be-giants/weep-day/verse_nokey.mid\n",
      "datasets/pianoroll/r/radiohead/pyramid-song/verse_nokey.mid\n",
      "datasets/pianoroll/j/jin/yobanashi-deceive/pre-chorus_nokey.mid\n",
      "datasets/pianoroll/l/lim-kim/awoo/pre-chorus_nokey.mid\n",
      "1.0\n",
      "datasets/pianoroll/j/jukebox-the-ghost/the-stars/chorus_nokey.mid\n",
      "datasets/pianoroll/t/they-might-be-giants/weep-day/verse_nokey.mid\n",
      "datasets/pianoroll/r/radiohead/pyramid-song/verse_nokey.mid\n",
      "datasets/pianoroll/j/jin/yobanashi-deceive/pre-chorus_nokey.mid\n",
      "datasets/pianoroll/l/lim-kim/awoo/pre-chorus_nokey.mid\n",
      "1.5\n",
      "datasets/pianoroll/j/jukebox-the-ghost/the-stars/chorus_nokey.mid\n",
      "datasets/pianoroll/t/they-might-be-giants/weep-day/verse_nokey.mid\n",
      "datasets/pianoroll/r/radiohead/pyramid-song/verse_nokey.mid\n",
      "datasets/pianoroll/j/jin/yobanashi-deceive/pre-chorus_nokey.mid\n",
      "datasets/pianoroll/l/lim-kim/awoo/pre-chorus_nokey.mid\n",
      "2.5\n",
      "datasets/pianoroll/j/jukebox-the-ghost/the-stars/chorus_nokey.mid\n",
      "datasets/pianoroll/t/they-might-be-giants/weep-day/verse_nokey.mid\n",
      "datasets/pianoroll/r/radiohead/pyramid-song/verse_nokey.mid\n",
      "datasets/pianoroll/j/jin/yobanashi-deceive/pre-chorus_nokey.mid\n",
      "datasets/pianoroll/l/lim-kim/awoo/pre-chorus_nokey.mid\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# random.shuffle(validation_idx_list)\n",
    "for TEMPERATURE in [None, 0.1, 0.25, 0.75, 1.0, 1.5, 2.5]:\n",
    "    print(TEMPERATURE)\n",
    "    songs_buffer = {}\n",
    "    # for iterlist in [train_idx_list, validation_idx_list]:\n",
    "    for iterlist in [validation_idx_list]:\n",
    "#         random.shuffle(iterlist)\n",
    "#         for i in range(len(iterlist)):\n",
    "        for i in range(min(len(iterlist), 5)):\n",
    "            sample_song_idx = iterlist[i]\n",
    "            # ABBA: 27\n",
    "            # ZUN: 86\n",
    "            # all: 2508, 8285\n",
    "\n",
    "            if sample_song_idx in train_idx_list:\n",
    "                dset_type = \"train\"\n",
    "            elif sample_song_idx in validation_idx_list:\n",
    "                dset_type = \"val\"\n",
    "            else:\n",
    "                dset_type = \"test\"\n",
    "            if TEMPERATURE:\n",
    "                out_fnam = f'{sample_song_idx}_{dset_type}_t={TEMPERATURE}.json'\n",
    "            else:\n",
    "                out_fnam = f'{sample_song_idx}_{dset_type}_t=None.json'\n",
    "\n",
    "            sample_chords = chords_translated[sample_song_idx]\n",
    "            first, _, indices_cache = data_utils.extract_first_last_by_song_idx(sample_song_idx, row_to_song_idx_train, indices_cache=indices_cache)\n",
    "            seed_n, _ = X_train_n_raw[first], X_train_ch_raw[first]\n",
    "            melody = pred_utils.predict_melody(\n",
    "                MODE,\n",
    "                seed_n, \n",
    "                sample_chords, \n",
    "                model, \n",
    "                id_to_token_pitches, \n",
    "                id_to_token_durations, \n",
    "                embeddings=EMBEDDINGS,\n",
    "                scaler=None,\n",
    "                notes_window=NOTES_WINDOW, \n",
    "                chords_window=CHORDS_WINDOW, \n",
    "                chord_size=PITCHES_PER_CHORD, \n",
    "                chords_mapping=chords_mapping,\n",
    "                temperature=TEMPERATURE\n",
    "            )\n",
    "\n",
    "            data = {\n",
    "                \"melody\": melody, \n",
    "                \"song\": TRAIN_SONGS[sample_song_idx]\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(predictions_dir, out_fnam), \"w\") as handle:\n",
    "                json.dump(\n",
    "                    data, \n",
    "                    handle\n",
    "                )\n",
    "\n",
    "            midi_stream = pred_utils.newsong_to_midistream(melody, sample_chords, tr_chords_dict=chords_mapping)\n",
    "            midi_stream.write(\n",
    "                'midi', \n",
    "                fp=os.path.join(predictions_dir, out_fnam.replace('.json', '.mid'))\n",
    "            )\n",
    "\n",
    "            print(TRAIN_SONGS[sample_song_idx])\n",
    "#             midi_stream.measures(0, 20).plot(\n",
    "#                 title=f'Sample song | Simple LSTM model trained with all artists songs\\nTemperature={TEMPERATURE}'\n",
    "#             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. UNK chords: 0\n",
      "# test SONGS: 1307\n"
     ]
    }
   ],
   "source": [
    "X_test_n_raw, X_test_ch_raw, y_test_raw, is_start_test, row_to_song_idx_test = data_utils.prepare_raw_X_y(\n",
    "    notes_translated_test, \n",
    "    chords_translated_test,\n",
    "    notes_window=NOTES_WINDOW,\n",
    "    chords_window=CHORDS_WINDOW,\n",
    "    use_next_chord=True,\n",
    "    rep=REP\n",
    ")\n",
    "\n",
    "y_test_raw_pitches, y_test_raw_durations = data_utils._split_notes(y_test_raw)\n",
    "y_test_raw_pitches, y_test_raw_durations = data_utils.trim_rare_labels_separately(\n",
    "    y_test_raw_pitches, y_test_raw_durations, \n",
    "    min_freq=MIN_FREQ, \n",
    "    unk=UNKNOWN\n",
    ")\n",
    "\n",
    "indices_cache_test = {}\n",
    "X_test, indices_cache_test = data_utils.prepare_input_X(\n",
    "    X_test_n_raw, \n",
    "    X_test_ch_raw, \n",
    "    is_start_test, \n",
    "    row_to_song_idx_test, \n",
    "    mode=MODE,\n",
    "    notes_window=NOTES_WINDOW, \n",
    "    chords_window=CHORDS_WINDOW,\n",
    "    chord_size=PITCHES_PER_CHORD,\n",
    "    chords_mapping=chords_mapping,\n",
    "    embeddings=EMBEDDINGS,\n",
    "    indices_cache=indices_cache_test\n",
    ")\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "y_test_pitches, _ = data_utils.prepare_input_y(y_test_raw_pitches, noneclass=NONECLASS)\n",
    "y_test_durations, _ = data_utils.prepare_input_y(y_test_raw_durations, noneclass=NONECLASS)\n",
    "\n",
    "test_song_idx_list = list(set(row_to_song_idx_test))\n",
    "print(\"# test SONGS:\", len(test_song_idx_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage/models/embeddings/LSTM-512_*/predictions\n",
      "None\n",
      "['C4_4.0', 'REST_4.0', 'G3_1.0', 'C4_0.5', 'D4_1.0', 'E4_1.5', 'B3_2.0', 'REST_8.0']\n",
      "datasets/pianoroll/a/a-great-big-world/say-something/verse_nokey.mid\n",
      "['C3_4.0', 'B3_4.0', 'C3_4.0', 'G3_1.5', 'A3_1.5', 'G3_1.0', 'C3_4.0', 'G3_4.0']\n",
      "datasets/pianoroll/a/a-ha/take-on-me/chorus_nokey.mid\n",
      "['F3_1.5', 'F3_1.0', 'E3_0.5', 'D3_1.0', 'REST_3.5', 'C3_0.5', 'E3_0.5', 'E3_1.0']\n",
      "datasets/pianoroll/a/a-ha/take-on-me/verse-and-pre-chorus_nokey.mid\n",
      "['REST_1.5', 'D5_1.5', 'A4_2.0', 'D5_1.0', 'A4_3.0', 'E4_2.0', 'A4_1.0', 'F4_3.0']\n",
      "datasets/pianoroll/a/a-perfect-circle/the-doomed/chorus_nokey.mid\n",
      "['REST_2.0', 'G4_1.0', 'A4_0.5', 'F#4_2.5', 'REST_4.0', 'G4_0.5', 'G4_1.0', 'G4_1.5']\n",
      "datasets/pianoroll/a/a-perfect-circle/the-nurse-who-loved-me/pre-chorus-and-chorus_nokey.mid\n",
      "0.1\n",
      "datasets/pianoroll/a/a-great-big-world/say-something/verse_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/verse-and-pre-chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-doomed/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-nurse-who-loved-me/pre-chorus-and-chorus_nokey.mid\n",
      "0.25\n",
      "datasets/pianoroll/a/a-great-big-world/say-something/verse_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/verse-and-pre-chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-doomed/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-nurse-who-loved-me/pre-chorus-and-chorus_nokey.mid\n",
      "0.75\n",
      "datasets/pianoroll/a/a-great-big-world/say-something/verse_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/verse-and-pre-chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-doomed/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-nurse-who-loved-me/pre-chorus-and-chorus_nokey.mid\n",
      "1.0\n",
      "datasets/pianoroll/a/a-great-big-world/say-something/verse_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/verse-and-pre-chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-doomed/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-nurse-who-loved-me/pre-chorus-and-chorus_nokey.mid\n",
      "1.5\n",
      "datasets/pianoroll/a/a-great-big-world/say-something/verse_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/verse-and-pre-chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-doomed/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-nurse-who-loved-me/pre-chorus-and-chorus_nokey.mid\n",
      "2.5\n",
      "datasets/pianoroll/a/a-great-big-world/say-something/verse_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/verse-and-pre-chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-doomed/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-nurse-who-loved-me/pre-chorus-and-chorus_nokey.mid\n",
      "10.0\n",
      "datasets/pianoroll/a/a-great-big-world/say-something/verse_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-ha/take-on-me/verse-and-pre-chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-doomed/chorus_nokey.mid\n",
      "datasets/pianoroll/a/a-perfect-circle/the-nurse-who-loved-me/pre-chorus-and-chorus_nokey.mid\n"
     ]
    }
   ],
   "source": [
    "print(predictions_dir)\n",
    "for TEMPERATURE in [None, 0.1, 0.25, 0.75, 1.0, 1.5, 2.5, 10.0]:\n",
    "    print(TEMPERATURE)\n",
    "    for iterlist in [test_song_idx_list]:\n",
    "    #     for i in range(len(iterlist)):\n",
    "        for i in range(5):\n",
    "            sample_song_idx = iterlist[i]\n",
    "            sample_chords = chords_translated_test[sample_song_idx]\n",
    "            first, _, indices_cache_test = data_utils.extract_first_last_by_song_idx(\n",
    "                sample_song_idx, row_to_song_idx_test, indices_cache=indices_cache_test\n",
    "            )\n",
    "            seed_n, _ = X_test_n_raw[first], X_test_ch_raw[first]\n",
    "            if not TEMPERATURE:\n",
    "                print(seed_n)\n",
    "\n",
    "            melody = pred_utils.predict_melody(\n",
    "                MODE,\n",
    "                seed_n, \n",
    "                sample_chords, \n",
    "                model, \n",
    "                id_to_token_pitches, \n",
    "                id_to_token_durations, \n",
    "                embeddings=EMBEDDINGS,\n",
    "                scaler=None,\n",
    "                notes_window=NOTES_WINDOW, \n",
    "                chords_window=CHORDS_WINDOW, \n",
    "                chord_size=PITCHES_PER_CHORD, \n",
    "                chords_mapping=chords_mapping,\n",
    "                temperature=TEMPERATURE\n",
    "            )\n",
    "            dset_type = \"test\"\n",
    "            \n",
    "            if TEMPERATURE:\n",
    "                out_fnam = f'{sample_song_idx}_{dset_type}_t={TEMPERATURE}.json'\n",
    "            else:\n",
    "                out_fnam = f'{sample_song_idx}_{dset_type}_t=None.json'\n",
    "\n",
    "#             with open(os.path.join(predictions_dir, f'{sample_song_idx}_{dset_type}.json'), \"w\") as handle:\n",
    "#                 json.dump(\n",
    "#                     {\"melody\": melody, \n",
    "#                      \"song\": TEST_SONGS[sample_song_idx]\n",
    "#                     }, \n",
    "#                     handle\n",
    "#                 )\n",
    "\n",
    "            midi_stream = pred_utils.newsong_to_midistream(melody, sample_chords, tr_chords_dict=chords_mapping)\n",
    "            outpath = os.path.join(predictions_dir, out_fnam.replace('json', 'mid'))\n",
    "            midi_stream.write(\n",
    "                'midi', \n",
    "                fp=outpath\n",
    "            )\n",
    "            print(TEST_SONGS[sample_song_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -cf $predictions_dir/pack_test.tar $predictions_dir/*test*.mid --newer=2020-11-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
